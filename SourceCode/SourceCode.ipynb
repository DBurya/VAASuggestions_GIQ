{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b908f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set random seed for reproducability\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED) \n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.enabled=True\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 85\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f8a23c",
   "metadata": {},
   "source": [
    "# Approach 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f4bfd",
   "metadata": {},
   "source": [
    "# Run topic modeling using party manifesto data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47318ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function for tokenizing Japanese party manifestos\n",
    "\n",
    "#Note: Since there are only few documents with many sentences, it's better to split them into the sentences and consider one sentence as one document.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import fugashi\n",
    "\n",
    "def tokenize_jp(text):\n",
    "    tagger = fugashi.Tagger()\n",
    "    words = [word.surface for word in tagger(text)]\n",
    "    return words\n",
    "\n",
    "#Topic-wise document grouping so later we can easily retrieve all documents per topic\n",
    "def groupdoc(topics,text):\n",
    "    topic_docs = {topic: [] for topic in set(topics)}\n",
    "    for topic, doc in zip(topics, text):\n",
    "        topic_docs[topic].append(doc)\n",
    "    del topic_docs[-1]\n",
    "    return topic_docs\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_jp)\n",
    "\n",
    "#Run topic modeling for party manifesto data\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "topicwiseGropedDocforAllModels = dict()\n",
    "topicwordsAllBertModelsPartyManifestos = dict()\n",
    "\n",
    "nt = ['20topics','25topics', '30topics','35topics','40topics','45topics','50topics','50topics',]\n",
    "r = 0\n",
    "\n",
    "#Starting number of topics\n",
    "n = 20\n",
    "\n",
    "#Name to add in the saved model\n",
    "name = 'PartyManifestosNew'\n",
    "\n",
    "#Set different parameter for HDBSCAN\n",
    "\n",
    "hdbscanmodel = hdbscan.HDBSCAN(min_cluster_size=10, cluster_selection_epsilon = 0.15, metric='euclidean', cluster_selection_method='eom',prediction_data=True)\n",
    "\n",
    "#Run topic modeling\n",
    "for i in nt[r]:\n",
    "    japanese = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "    topic_el = BERTopic(embedding_model=japanese, top_n_words=10, hdbscan_model=hdbscanmodel, nr_topics=n, vectorizer_model=vectorizer,calculate_probabilities=False, low_memory = True)    \n",
    "    topics, _ = topic_el.fit_transform(YouTUbeData)\n",
    "#Topic_el.save('Bert' + name + nt[r])\n",
    "    b = groupdoc(topics,YouTUbeData)\n",
    "    a = extract_words(topic_el,n)\n",
    "    topicwiseGropedDocforAllModels[n] = b\n",
    "    topicwordsAllBertModelsPartyManifestos[n] = a\n",
    "    n += 7\n",
    "    if r !=len(nt) - 1:\n",
    "        r += 1\n",
    "    else:     \n",
    "         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324d50a",
   "metadata": {},
   "source": [
    "# Calculate coherence score (Cv) to select one topic model \n",
    "Need tokens and dictionary which is  {word: frequency}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ae276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unction for tokenization\n",
    "words=[]\n",
    "tagger = fugashi.Tagger()\n",
    "def tokenize_p(docs):\n",
    "    word = [word.surface for word in tagger(docs)]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ed7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for tokenizing set of documant for every topic\n",
    "#Remove stopwords\n",
    "\n",
    "def funcdoc(text):\n",
    "    stopwordsJp = [line.rstrip('\\n') for line in open (r\"C:\\Users\\esoc-ws\\Desktop\\Research\\Journal_2022\\stopwords-jp.txt\", encoding='utf-8')]\n",
    "    \n",
    "    tok=dict()\n",
    "    nr = int(text[-8:-6])\n",
    "    for i in range(nr):\n",
    "        splist=[]\n",
    "        for h in range(len(topicwiseGropedDocforAllModels[nr][i])):\n",
    "                b = tokenize_p(topicwiseGropedDocforAllModels[nr][i][h])\n",
    "                splist.append(b)\n",
    "                \n",
    "                \n",
    "        tok[i] = splist\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06363e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for tokenizing set of documant for every topic\n",
    "\n",
    "def funcdoc(text):\n",
    "    stopwordsJp = [line.rstrip('\\n') for line in open (\"stopwords-jp.txt\", encoding='utf-8')]\n",
    "    tok=dict()\n",
    "    nr = int(text[-8:-6])\n",
    "    for i in range(nr):\n",
    "        splist=[]\n",
    "        for h in range(len(topicwiseGropedDocforAllModels[nr][i])):\n",
    "                b = tokenize_p(topicwiseGropedDocforAllModels[nr][i][h])\n",
    "                for s in b:\n",
    "                    if s in stopwordsJp:\n",
    "                        b.remove(s)\n",
    "                splist.append(b)    \n",
    "        tok[i] = splist\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5074b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to delete stopwords from dict\n",
    "def cleandict(dictJP):\n",
    "    stopwordsJp = [line.rstrip('\\n') for line in open (\"stopwords-jp.txt\", encoding='utf-8')]\n",
    "    for i in range(len(dictJP)):\n",
    "        for b in dictJP[i]:\n",
    "            if b in stopwordsJp:\n",
    "                dictJP[i].remove(b)\n",
    "    return dictJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff1009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Names of all saved models\n",
    "listofBertNames = ['BertPartyManifestosNew20topics','BertPartyManifestosNew25topics','BertPartyManifestosNew30topics',\n",
    "                   'BertPartyManifestosNew35topics','BertPartyManifestosNew40topics',\n",
    "                   'BertPartyManifestosNew45topics','BertPartyManifestosNew50topics',\n",
    "                   'BertPartyManifestosNew55topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate coherence score\n",
    "def calculate_C_S():\n",
    "    resList = list()\n",
    "    from gensim.test.utils import common_corpus\n",
    "    from gensim.models.coherencemodel import CoherenceModel\n",
    "    from gensim.corpora import Dictionary \n",
    "#Tokenizing documentssnand crating gensim dictionary for corpus\n",
    "    for i in listofBertNames:\n",
    "        tok = funcdoc(i)\n",
    "        total_coherence = 0\n",
    "\n",
    "        for b in range(len(tok)):\n",
    "            a = cleandict(tok[b])\n",
    "            JpDict = Dictionary(a)\n",
    "            JpDict.token2id\n",
    "            \n",
    "            \n",
    "#Calculate topic-wise coherence score and average it for every BertModel\n",
    "\n",
    "            \n",
    "            topics = [topicwordsAllBertModelsPartyManifestos[int(i[-8:-6])][b]]\n",
    "            cm = CoherenceModel(topics=topics, texts=tok[b], dictionary=JpDict, coherence='c_v')\n",
    "#Get coherence value\n",
    "            coherence = cm.get_coherence()\n",
    "            total_coherence += coherence\n",
    "                \n",
    "        AverC_V = total_coherence / int(i[-8:-6])\n",
    "        print(b, AverC_V)\n",
    "        resList.append(AverC_V)\n",
    "            \n",
    "            \n",
    "    return resList\n",
    "\n",
    "\n",
    "resList = calculate_C_S()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea17ed",
   "metadata": {},
   "source": [
    "# Calculate similarity score between party manifestos and VAA statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c075600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract topic-wise documents\n",
    "topicwise_documents = topicwiseGropedDocforAllModels[45]\n",
    "len(topicwise_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4615e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output the topic-documents pairs with their score\n",
    "def calculate_cossim(sentences1,sentences2):\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "    #Compute embeddings for both lists\n",
    "    global score\n",
    "    score={}\n",
    "    embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "    for i in range(len(sentences2)):\n",
    "        embeddings2 = (model.encode(sentences2[i], convert_to_tensor=True))\n",
    "    #Compute cosine-similarity and use probability\n",
    "        global cosine_scores\n",
    "        cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "        for f in range(len(sentences1)):\n",
    "            cosine_sim=0\n",
    "            for s in range(len(sentences2[i])):\n",
    "                cosine_sim+=cosine_scores[f][s]\n",
    "            score[sentences1[f],i]=cosine_sim/len(sentences2[i])\n",
    "            print('''{} \\n {} \\n\\x1b[31m\\\"SCORE\\\"\\x1b[0m: {:.4f}'''.format(sentences1[f], i, score[sentences1[f],i]))\n",
    "            #print(sentences1[f]\\n\\n, sentences2[i], '\\033[91m{SCORE}\\033[0m', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579df9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = VAA_statements\n",
    "\n",
    "sentences2 = topicwise_documents\n",
    "\n",
    "calculate_cossim(sentences1,sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae964b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average cossim score for every VAA statement\n",
    "dictVaaScore={}\n",
    "\n",
    "for st in range(len(VAAStat)):\n",
    "    average = 0\n",
    "    finalscore = 0\n",
    "    for i in range(len(topicwise_documents)):\n",
    "        average+=np.asarray(score[VAAStat[st],i]).tolist()\n",
    "    finalscore = average / len(topicwise_documents)\n",
    "    dictVaaScore[VAAStat[st]] = finalscore\n",
    "    \n",
    "dictVaaScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ec202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leave only document-statement pairs which cosine similarity score is above 0.5\n",
    "dictwithresults={}\n",
    "for k in dictVaaScore.keys():\n",
    "    for i in range(35):\n",
    "        if score[k,i] > 0.5:\n",
    "            dictwithresults[VAAStat[VAAStat.index(k)]]= score[k,i],i\n",
    "            \n",
    "\n",
    "dictwithresults    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ffeacb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here need to merge some VAA statements since some of them are very similar, and recalculate cosine similarity with the selected documents\n",
    "\n",
    "#Same approach should be used with YouTube comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11da3c1",
   "metadata": {},
   "source": [
    "# Approach 2\n",
    "In addition to approach 1, if documents needs to be summarized to save time of VAA designers, or they would like to consider only the most representative documents the following additional feature can be added to the system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In BERT there is a method to retrieve the most representative documents from each topic\n",
    "\n",
    "representative_documents = topic_el.model.get_representative_docs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ae52e",
   "metadata": {},
   "source": [
    "# Bert extractive summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c733425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simmarize documents\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
    "from summarizer import Summarizer\n",
    "#In order to make method work properly, \"sentence_handler.py\" from the Summarizer library had to be adjusted for for the specific language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb032722",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = AutoConfig.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "custom_config.output_hidden_states=True\n",
    "custom_tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\")\n",
    "custom_model = AutoModel.from_pretrained(\"cl-tohoku/bert-base-japanese\", config=custom_config)\n",
    "\n",
    "model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622af4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ce0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95a526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e2607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49abc7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
